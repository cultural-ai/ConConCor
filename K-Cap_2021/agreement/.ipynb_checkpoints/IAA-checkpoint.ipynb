{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating inter-annotators agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script generates 8 additional files:\n",
    "1. batches_annotators.json – a list of annotators per batch\n",
    "2. k_alpha_per_batch_4_options.csv – Krippendorff's alpha per batch for all 4 options\n",
    "3. k_alpha_per_batch_2_options.csv – Krippendorff's alpha per batch for 2 options ('Omstreden' and 'Niet omstreden'), other responses are filtered out, so this data has missing values; the purpose of it is to check the agreement between annotators who could decide whether a term was contentious or non-contentious in a given sample (without options 'I don't know' or 'Bad OCR')\n",
    "4. pairwise_agreement.csv – Krippendorff's alpha for every pair of annotators in a batch\n",
    "5. mean_alpha_per_annotator.csv – mean Krippendorff's alpha per annotator (taking all the alpha values from an annotator's pairs)\n",
    "6. perc_agreement.csv – percentage agreement between annotators per sample \n",
    "7. k_alpha_per_batch_2_options_filtered_alpha.csv – Krippendorff's alpha per batch for 2 options without annotators whose mean K alpha lower than 0.2\n",
    "8. k_alpha_per_batch_2_options_filtered_controls.csv – Krippendorff's alpha per batch for 2 options without annotators who got 3 or more control questions 'wrong' (different from experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing csv files from  GitHub\n",
    "\n",
    "url_1 = \"#\" # link to the raw file (https://raw.githubusercontent.com/cultural-ai/ConConCor/master/Dataset/Annotations.csv)\n",
    "annotations = requests.get(url_1).content\n",
    "\n",
    "url_2 = \"#\" # link to the raw file (https://raw.githubusercontent.com/cultural-ai/ConConCor/master/Dataset/Extracts.csv)\n",
    "extracts = requests.get(url_2).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# putting the csv data into pandas df \n",
    "annotations_data = pd.read_csv(io.StringIO(annotations.decode('utf-8')))\n",
    "extracts_data = pd.read_csv(io.StringIO(extracts.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selecting relevant columns\n",
    "annotations_asr = annotations_data[['anonymised_participant_id','extract_id','response']]\n",
    "extracts_et = extracts_data[['extract_id','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dfs\n",
    "annotations_with_target = pd.merge(annotations_asr, extracts_et, how='inner', on=['extract_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# converting df to a list\n",
    "list_merged = annotations_with_target.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making a list of unique extract IDs\n",
    "list_of_extracts = extracts_data['extract_id']\n",
    "list_of_unique_extracts = list(Counter(list_of_extracts).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list of unique annotators\n",
    "list_of_unique_annotators = list(annotations_asr.groupby('anonymised_participant_id').groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grouping by 'extract_id' to see how many times the same extracts were annotated\n",
    "groups_per_extract = dict(annotations_asr.groupby('extract_id').groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_anns = {} # dict with 'extract_id':'number of annotations'\n",
    "for ext in groups_per_extract:\n",
    "    anns_num = len(groups_per_extract[ext]) # counting the number of annotations per sample\n",
    "    sample_anns[ext] = anns_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting groups of annotators (batches) (batches_annotators.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* if the annotators are in one group it means that they annotated the same set of samples (a batch)\n",
    "* we need to have the annotators grouped for calculating K alpha per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking which samples every participant annotated\n",
    "\n",
    "annotator_extracts = {} #dict with 'anonymised_participant_id': list of extracts they annotated\n",
    "for group in annotations_asr.groupby('anonymised_participant_id'):\n",
    "    annotator_extracts[group[0]] = list(group[1]['extract_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finding unique batches of samples\n",
    "# lists of samples are transformed to str to use Counter\n",
    "\n",
    "unique_batches = list(Counter(str(e) for e in list(annotator_extracts.values())).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group_annotators = {} # dict 'group': list of annotators in the group\n",
    "group_counter = 0\n",
    "for unique in unique_batches: # iterating over the list of unique batches\n",
    "    annotators_list = []\n",
    "    group_counter += 1\n",
    "    for antr in annotator_extracts: # checking if the unique batch matches the list of extracts\n",
    "        if str(annotator_extracts[antr]) == unique:\n",
    "            annotators_list.append(antr) # putting annotators in the same group in a list \n",
    "    group_name = f\"batch_{group_counter}\"\n",
    "    group_annotators[group_name] = annotators_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting lists of annotators by batches in json\n",
    "with open('batches_annotators.json', 'w') as outfile:\n",
    "    json.dump(group_annotators, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Krippendorff's alpha per group (k_alpha_per_batch_4_options.csv) (all options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* to calculate K alpha we need tuples with ('anonymised_participant_id','extract_id','response') for every batch\n",
    "* we take all 4 options (Contentious, Non-contentious, I don't know, Bad OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# converting df to a list with all responses\n",
    "triples_results = annotations_asr.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a csv with aplha scores per group and num of annotators\n",
    "\n",
    "with open('k_alpha_per_batch_4_options.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['batch', 'k_alpha_4', 'num_annotators']) #header\n",
    "    \n",
    "    for group in group_annotators: # iterating over groups\n",
    "        responses_list = [] # list to store tuples for every group\n",
    "\n",
    "        for triple in triples_results: # iterating over responses\n",
    "            if triple[0] in group_annotators[group]: # collecting tuples for every group\n",
    "                responses_tuple = (triple[0],triple[1],triple[2])\n",
    "                responses_list.append(responses_tuple)\n",
    "\n",
    "        try:\n",
    "            t = AnnotationTask(data=responses_list)\n",
    "            k_alpha = round(t.alpha(),3)\n",
    "\n",
    "        except ZeroDivisionError: # batch_50 has only 1 annotator\n",
    "            k_alpha = 'zero division'\n",
    "\n",
    "        writer.writerow([group,k_alpha,len(group_annotators[group])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Krippendorff's alpha per group (k_alpha_per_batch_2_options.csv) (only omstreden/niet omstreden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* to calculate K alpha we need tuples with ('anonymised_participant_id','extract_id','response') for every batch\n",
    "* we filter out (1) all the extracts (for every annotator in a group) with less than 2 options 'Omstreden' or 'Niet omstreden' in every batch (117) (it is necessary for calculating alpha correctly) and (2) the extracts with options 'Weet ik niet' and 'Onleesbare OCR' (3523), in total of 3700 extracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out the extracts with less than 2 options 'Omstreden' or 'Niet omstreden' in a batch\n",
    "\n",
    "extracts_to_filter = [] # storing the extracts to be filtered out (negative list)\n",
    "\n",
    "for unique_extract_id in list_of_unique_extracts: # iterating over the list of unique extracts IDs\n",
    "        list_of_responses_per_extract = [] # collecting responses for every extract\n",
    "        \n",
    "        for row in list_merged: # iterating over the list with all responses\n",
    "            if unique_extract_id == row[1]: # matching with the unique extract ID\n",
    "                list_of_responses_per_extract.append(row[2]) # adding all the responses per single extract\n",
    "                extract_id = row[1] # saving the extract ID\n",
    "                \n",
    "        # counting the 2 options in the responses list per extract\n",
    "        opt_1 = list_of_responses_per_extract.count('Omstreden naar huidige maatstaven')\n",
    "        opt_2 = list_of_responses_per_extract.count('Niet omstreden')\n",
    "        \n",
    "        check_sum = opt_1 + opt_2\n",
    "        \n",
    "        if check_sum < 2: # the sum of 2 options should be no less than 2\n",
    "            extracts_to_filter.append(extract_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_results_filtered = [] # the list of the filtered responses (18100)\n",
    "for triple in triples_results: # the original results list (non-filtered) (21800)\n",
    "     \n",
    "    # checking if the 'extract_id' is not on the negative list AND (117 in the negative list)\n",
    "    # filtering out 'Weet ik niet' and 'Onleesbare OCR' options (3583)\n",
    "    if triple[1] not in extracts_to_filter and triple[2] != 'Weet ik niet' and triple[2] != 'Onleesbare OCR':\n",
    "        triples_results_filtered.append(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a csv with aplha scores per group and num of annotators (for 2 options)\n",
    "\n",
    "with open('k_alpha_per_batch_2_options.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['batch', 'k_alpha_2', 'num_annotators']) #header\n",
    "    \n",
    "    for group in group_annotators: # iterating over groups\n",
    "        responses_list = [] # list to store tuples for every group\n",
    "\n",
    "        for triple in triples_results_filtered: # iterating over responses\n",
    "            \n",
    "            if triple[0] in group_annotators[group]: # collecting tuples for every group\n",
    "                responses_tuple = (triple[0],triple[1],triple[2])\n",
    "                responses_list.append(responses_tuple)\n",
    "\n",
    "        try:\n",
    "            t = AnnotationTask(data=responses_list)\n",
    "            k_alpha = round(t.alpha(),3)\n",
    "\n",
    "        except ZeroDivisionError: # batch_50 has only 1 annotator\n",
    "            k_alpha = 'zero division'\n",
    "        \n",
    "        writer.writerow([group,k_alpha,len(group_annotators[group])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairwise agreement (pairwise_alpha.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('pairwise_alpha.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['batch', 'annotator_1', 'annotator_2', 'alpha']) #header\n",
    "    \n",
    "    for group in group_annotators: # iterating over annotators' groups\n",
    "        pairs_in_group = list(combinations(group_annotators[group], 2)) # pairs combinations in every group\n",
    "\n",
    "        for pair in pairs_in_group: # iterating over every pair of annotators in a group\n",
    "            responses_list = [] # all the responses for 2 annotators in a pair\n",
    "\n",
    "            for triple in triples_results: # iterating over the responses\n",
    "                \n",
    "                if pair[0] == triple[0]: # matching annotators' IDs\n",
    "                    responses_list.append((triple[0],triple[1],triple[2])) # putting all the responses of annotator_1  in a tuple\n",
    "\n",
    "                if pair[1] == triple[0]:\n",
    "                    responses_list.append((triple[0],triple[1],triple[2])) # putting all the responses of annotator_2  in a tuple\n",
    "\n",
    "            # alpha for every pair in the group\n",
    "            \n",
    "            try:\n",
    "                t = AnnotationTask(data=responses_list)\n",
    "                k_alpha = round(t.alpha(),3)\n",
    "\n",
    "            except ZeroDivisionError:\n",
    "                k_alpha = 'zero division'\n",
    "\n",
    "            writer.writerow([group,pair[0],pair[1],k_alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean K alpha for every annotator (mean_alpha_per_annotator.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading 'pairwise_alpha.csv'\n",
    "\n",
    "pairwise_agreement = pd.read_csv(\"pairwise_alpha.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_in_tuples = [] # list with tuples (pairs of annotators) and alpha per pair\n",
    "\n",
    "for i,row in pairwise_agreement.iterrows():\n",
    "    pairs_in_tuples.append([(row['annotator_1'],row['annotator_2']),row['alpha']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_mean = [] # list of mean alpha per annotator\n",
    "for unique in list_of_unique_annotators: # iterating over the list of unique annotators\n",
    "    ann_values = []\n",
    "    for pair in pairs_in_tuples:\n",
    "        if unique in pair[0]: # matching annotators IDs\n",
    "            ann_values.append(float(pair[1])) # putting alpha of single annotator in a list\n",
    "            \n",
    "    if ann_values != []: # there's one batch with only 1 annotator, so there's no pair\n",
    "        mean_kappa = round(statistics.mean(ann_values),3)\n",
    "        ann_mean.append([unique,mean_kappa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the csv\n",
    "\n",
    "mean_alpha = pd.DataFrame(ann_mean,columns=['anonymised_participant_id','mean_alpha'])\n",
    "mean_alpha.to_csv('mean_alpha_per_annotator.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentage agreement (perc_agreement.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating 'perc_agreement.csv' with count by responses per extract and % agreement\n",
    "\n",
    "with open('perc_agreement.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['extract_id', 'omstreden', 'niet_omstreden',\n",
    "                     'weet_ik_niet', 'bad_ocr', 'num_annotators', '%_agree']) #header\n",
    "    \n",
    "    for unique_extract_id in list_of_unique_extracts: # iterating over the list of unique extracts IDs\n",
    "        list_of_responses_per_extract = [] # collecting responses for every extract\n",
    "        \n",
    "        for row in list_merged: # iterating over the list with all responses\n",
    "            if unique_extract_id == row[1]: # matching with the unique extract ID\n",
    "                list_of_responses_per_extract.append(row[2]) # adding all the responses per single extract\n",
    "                extract_id = row[1] # saving the extract ID\n",
    "                target = row[3] # saving the target word of the extract\n",
    "                \n",
    "        # counting every option in the responses list per extract\n",
    "        opt_1 = list_of_responses_per_extract.count('Omstreden naar huidige maatstaven')\n",
    "        opt_2 = list_of_responses_per_extract.count('Niet omstreden')\n",
    "        opt_3 = list_of_responses_per_extract.count('Weet ik niet')\n",
    "        opt_4 = list_of_responses_per_extract.count('Onleesbare OCR')\n",
    "        \n",
    "        num_ann = len(list_of_responses_per_extract) # number of annotators per extract\n",
    "        perc_agree = round(max(opt_1,opt_2,opt_3,opt_4)/num_ann * 100) # % agreement\n",
    "        extract_name = f\"{target}_{extract_id}\" # giving to an extract a new ID with its target word \n",
    "        result_row = [extract_name,opt_1,opt_2,opt_3,opt_4,num_ann,perc_agree]\n",
    "        \n",
    "        writer.writerow(result_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does median K alpha per batch change when annotators with mean alpha < 0.2 are excluded? (k_alpha_per_batch_2_options_filtered_alpha.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of annotators with alpha < 0.2\n",
    "low_alpha_annotators = []\n",
    "for index,row in mean_alpha.iterrows():\n",
    "    if row['mean_alpha'] < 0.2:\n",
    "        low_alpha_annotators.append(str(row['anonymised_participant_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a duplicate of group_annotators to remove annotators with low alpha\n",
    "no_low_alpha = group_annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding the low alpha annotators from the batches they annotated\n",
    "for ids in low_alpha_annotators:\n",
    "    for group in no_low_alpha:\n",
    "        if ids in no_low_alpha[group]:\n",
    "            no_low_alpha[group].remove(ids) # ! it changes no_low_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-running calculation of K alpha for 2 options without the low alpha annotators\n",
    "\n",
    "# creating a csv with aplha scores per group and num of annotators (for 2 options)\n",
    "# without annotators with mean alpha < 0.2\n",
    "\n",
    "with open('k_alpha_per_batch_2_options_filtered_alpha.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['batch', 'k_alpha_2', 'num_annotators']) #header\n",
    "    \n",
    "    for group in no_low_alpha: # iterating over groups\n",
    "        responses_list = [] # list to store tuples for every group\n",
    "\n",
    "        for triple in triples_results_filtered: # iterating over responses\n",
    "            \n",
    "            if triple[0] in no_low_alpha[group]: # collecting tuples for every group\n",
    "                responses_tuple = (triple[0],triple[1],triple[2])\n",
    "                responses_list.append(responses_tuple)\n",
    "\n",
    "        try:\n",
    "            t = AnnotationTask(data=responses_list)\n",
    "            k_alpha = round(t.alpha(),3)\n",
    "\n",
    "        except ZeroDivisionError: # batches with only 1 annotator\n",
    "            k_alpha = 'zero division'\n",
    "        \n",
    "        writer.writerow([group,k_alpha,len(no_low_alpha[group])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does median K alpha change when excluding annotators whose responses to the 5 control questions differ from the unanimous responses of experts (3 or more questions)? (k_alpha_per_batch_2_options_filtered_controls.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathering all the responses to control samples\n",
    "\n",
    "all_control_responses = []\n",
    "\n",
    "for response in triples_results: # list of responses\n",
    "    if 'c' in response[1]: #c is a prefix for control samples\n",
    "        all_control_responses.append([response[0],(response[1],response[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses of experts to control questions with 100% agreement\n",
    "unanimous_responses = [('c0','Omstreden naar huidige maatstaven'),\n",
    "                      ('c1','Omstreden naar huidige maatstaven'),\n",
    "                      ('c2','Niet omstreden'),\n",
    "                      ('c3','Omstreden naar huidige maatstaven'),\n",
    "                      ('c4', 'Niet omstreden')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_controls = []\n",
    "for response in all_control_responses:\n",
    "    if response[1] not in unanimous_responses:\n",
    "        wrong_controls.append(response[0]) # IDs of annotators who got the controls 'wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators_to_filter = []\n",
    "for i in dict(Counter(wrong_controls)):\n",
    "    if dict(Counter(wrong_controls))[i] >= 3: # 3 or more 'wrong' controls\n",
    "        annotators_to_filter.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a duplicate of group_annotators to remove annotators who got 3 or more controls 'wrong'\n",
    "no_wrong_controls = group_annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding the annotators with 'wrong' controls from the batches they annotated\n",
    "for ids in annotators_to_filter:\n",
    "    for group in no_wrong_controls:\n",
    "        if ids in no_wrong_controls[group]:\n",
    "            no_wrong_controls[group].remove(ids) # ! it changes no_wrong_controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-running calculation of K alpha for 2 options without the annotators with 'wrong' controls\n",
    "\n",
    "# creating a csv with aplha scores per group and num of annotators (for 2 options)\n",
    "# without annotators with 'wrong' controls\n",
    "\n",
    "with open('k_alpha_per_batch_2_options_filtered_controls.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['batch', 'k_alpha_2', 'num_annotators']) #header\n",
    "    \n",
    "    for group in no_wrong_controls: # iterating over groups\n",
    "        responses_list = [] # list to store tuples for every group\n",
    "\n",
    "        for triple in triples_results_filtered: # iterating over responses\n",
    "            \n",
    "            if triple[0] in no_wrong_controls[group]: # collecting tuples for every group\n",
    "                responses_tuple = (triple[0],triple[1],triple[2])\n",
    "                responses_list.append(responses_tuple)\n",
    "\n",
    "        try:\n",
    "            t = AnnotationTask(data=responses_list)\n",
    "            k_alpha = round(t.alpha(),3)\n",
    "\n",
    "        except ZeroDivisionError: # batches with only 1 annotator\n",
    "            k_alpha = 'zero division'\n",
    "        \n",
    "        writer.writerow([group,k_alpha,len(no_wrong_controls[group])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_per_batch = pd.read_csv('k_alpha_per_batch_2_options.csv')\n",
    "alpha_per_batch_filtered_low_alpha = pd.read_csv('k_alpha_per_batch_2_options_filtered_alpha.csv')\n",
    "alpha_per_batch_filtered_controls = pd.read_csv('k_alpha_per_batch_2_options_filtered_controls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing medians\n",
    "\n",
    "for i,row in alpha_per_batch.iterrows():\n",
    "    if (row['k_alpha_2']) == 'zero division':\n",
    "        alpha_per_batch.drop([i], axis=0, inplace=True)\n",
    "        \n",
    "for i,row in alpha_per_batch_filtered_low_alpha.iterrows():\n",
    "    if (row['k_alpha_2']) == 'zero division':\n",
    "        alpha_per_batch_filtered_low_alpha.drop([i], axis=0, inplace=True)\n",
    "        \n",
    "for i,row in alpha_per_batch_filtered_controls.iterrows():\n",
    "    if (row['k_alpha_2']) == 'zero division':\n",
    "        alpha_per_batch_filtered_controls.drop([i], axis=0, inplace=True)\n",
    "        \n",
    "print(f\"Median K alpha:{alpha_per_batch['k_alpha_2'].median()}\\n Median K alpha (no annotators with a < 0.2): {alpha_per_batch_filtered_low_alpha['k_alpha_2'].median()}\\n Median K alpha (no annotators with 3 or more 'wrong' controls): {alpha_per_batch_filtered_controls['k_alpha_2'].median()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
