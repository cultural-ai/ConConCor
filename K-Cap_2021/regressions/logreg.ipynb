{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5623fba",
   "metadata": {},
   "source": [
    " 1. Baselines\n",
    " 2. LogReg with target + annotator\n",
    " 3. LogReg with context + annotator\n",
    " 4. LogReg with target + context + similarity + annotator\n",
    "\n",
    "\n",
    " 5. & 6. & 7.: kNN with 2., 3. and 4. \n",
    "\n",
    "\n",
    "### DONE: LogReg with Annotators\n",
    "\n",
    " - group by annotators -> take majority vote across contexts\n",
    " - add annotator (categorical variable) as a predictor\n",
    " \n",
    " \n",
    "### SVD Grid Search\n",
    "\n",
    " - reduce tf-idf space to > 100 dims (256?) = M \\in V x 256\n",
    " - loop over dimensions k and produce N \\n V x k from M\n",
    " - fit LogReg on N\n",
    " \n",
    " \n",
    " \n",
    "### DONE: use Word2Vec embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### DONE: split train-test by target words\n",
    "\n",
    " - measure accuracy per annotator, then average (happens by default)\n",
    " - prediction for target is average  \n",
    " \n",
    " \n",
    " \n",
    "### DONE: adapt baseline\n",
    "\n",
    " - old: majority vote on the most common class *across all samples*\n",
    " \n",
    " - new: majority vote on most common class *per annotator* -> annotator prior P(c|a)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "=> P(c | w, a) = P(c | (x_1, ..., x_k), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0790c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from utils import get_DF, get_indices\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, accuracy_score,\\\n",
    "                                    confusion_matrix, plot_roc_curve, precision_score, recall_score,\\\n",
    "                                    f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b00126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_DF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fdf1dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = joblib.load(f'vectors/tfidf_0.15.pkl')\n",
    "mat = pd.read_csv(\"vectors/svd_mat_0.15.tsv\", delimiter=\"\\t\", header=None).to_numpy()\n",
    "scaled = StandardScaler(with_mean=False).fit_transform(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f26d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:00<00:00, 108.42it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf_words = tfidf.get_feature_names()\n",
    "mat_inds = get_indices(df.target.unique(), tfidf_words)\n",
    "df[\"mat_ind\"] = df.target.apply(lambda w: mat_inds[w] if w in mat_inds else -1)\n",
    "df = df[df.mat_ind > -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca7d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_annotations = {x:list(subdf.y) for x, subdf in df.groupby([\"annotator_x\", \"target\"])}\n",
    "xs, y = list(zip(*target_annotations.items()))\n",
    "x_anno, x_target = list(zip(*xs))\n",
    "\n",
    "mldf = pd.DataFrame([x_anno, x_target, y]).T\n",
    "mldf.columns = [\"x_anno\", \"target\", \"y_ls\"]\n",
    "\n",
    "mldf[\"ratio\"] = mldf.y_ls.apply(lambda ls: sum(ls)/len(ls))\n",
    "mldf = mldf[mldf.ratio != 0.5]\n",
    "mldf[\"maj_vote\"] = mldf.ratio.apply(lambda r: int(r > 0.5))\n",
    "\n",
    "mldf = pd.concat([mldf, pd.get_dummies(mldf[\"x_anno\"])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce38d05",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ebaaf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([np.stack(mldf.target.apply(lambda w: scaled[mat_inds[w]])), \n",
    "               mldf.iloc[:, 5:].to_numpy()])\n",
    "Y = mldf.maj_vote.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "446e3094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  30 out of  30 | elapsed:  9.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([156.22908854, 159.58916569, 155.18634486, 158.04874444,\n",
       "        161.35234809, 154.26207781, 151.87549663, 158.20490623,\n",
       "        155.26145697, 154.13900995, 155.82272816, 156.33400655,\n",
       "        156.37657928, 153.34250975, 153.01957703, 151.35069275,\n",
       "        160.08963442, 159.63079119, 156.59928894, 156.99560738,\n",
       "        157.63155627, 157.2330513 , 157.11628962, 156.68600726,\n",
       "        115.6810472 , 114.17920399, 117.57854772, 110.54601145,\n",
       "        116.22277141, 112.48255348]),\n",
       " 'score_time': array([0.06067729, 0.02589893, 0.0295558 , 0.03180718, 0.04213858,\n",
       "        0.03476596, 0.03742743, 0.0215559 , 0.02864194, 0.03003454,\n",
       "        0.02835989, 0.07665181, 0.0270505 , 0.03226566, 0.02795029,\n",
       "        0.02033901, 0.03510976, 0.02304363, 0.0307951 , 0.03810954,\n",
       "        0.0198915 , 0.06029248, 0.02295423, 0.02596164, 0.01034284,\n",
       "        0.01417041, 0.01023054, 0.01420212, 0.00566816, 0.01360798]),\n",
       " 'test_accuracy': array([0.78525641, 0.59615385, 0.75961538, 0.78846154, 0.63782051,\n",
       "        0.63782051, 0.65064103, 0.71474359, 0.71153846, 0.57371795,\n",
       "        0.74679487, 0.58653846, 0.72115385, 0.71474359, 0.60771704,\n",
       "        0.65594855, 0.65916399, 0.73954984, 0.81993569, 0.73954984,\n",
       "        0.64951768, 0.46945338, 0.21221865, 0.4244373 , 0.65916399,\n",
       "        0.57234727, 0.60771704, 0.7266881 , 0.59485531, 0.67202572]),\n",
       " 'test_balanced_accuracy': array([0.71180556, 0.60416667, 0.65277778, 0.73148148, 0.59085648,\n",
       "        0.59375   , 0.61458333, 0.6724537 , 0.67592593, 0.55034722,\n",
       "        0.65798611, 0.62037037, 0.61921296, 0.66377315, 0.60555556,\n",
       "        0.61374269, 0.62195419, 0.6621345 , 0.77012671, 0.6621345 ,\n",
       "        0.56488791, 0.41461988, 0.23827973, 0.42054094, 0.60646802,\n",
       "        0.53791182, 0.57214147, 0.67260174, 0.54554264, 0.65324612]),\n",
       " 'test_f1_weighted': array([0.77505652, 0.61163227, 0.73416072, 0.78346272, 0.6429017 ,\n",
       "        0.64368572, 0.65769386, 0.71669348, 0.71526081, 0.58779033,\n",
       "        0.73089461, 0.60144976, 0.69887147, 0.71432624, 0.62257363,\n",
       "        0.66162767, 0.6658458 , 0.72940495, 0.81629941, 0.72940495,\n",
       "        0.64037759, 0.48477485, 0.2195865 , 0.44663639, 0.66104174,\n",
       "        0.58438359, 0.61730486, 0.72460496, 0.60180502, 0.68103019]),\n",
       " 'test_precision_weighted': array([0.7768507 , 0.66153846, 0.75035107, 0.78203769, 0.64947835,\n",
       "        0.65164036, 0.66842381, 0.71901113, 0.72043344, 0.6152808 ,\n",
       "        0.73223295, 0.68055613, 0.69996547, 0.71392377, 0.66277139,\n",
       "        0.66949315, 0.67586107, 0.7267057 , 0.81546222, 0.7267057 ,\n",
       "        0.63412693, 0.50607318, 0.29208175, 0.50703911, 0.66312918,\n",
       "        0.6043385 , 0.63255385, 0.72290973, 0.61096011, 0.69907341]),\n",
       " 'test_recall_weighted': array([0.78525641, 0.59615385, 0.75961538, 0.78846154, 0.63782051,\n",
       "        0.63782051, 0.65064103, 0.71474359, 0.71153846, 0.57371795,\n",
       "        0.74679487, 0.58653846, 0.72115385, 0.71474359, 0.60771704,\n",
       "        0.65594855, 0.65916399, 0.73954984, 0.81993569, 0.73954984,\n",
       "        0.64951768, 0.46945338, 0.21221865, 0.4244373 , 0.65916399,\n",
       "        0.57234727, 0.60771704, 0.7266881 , 0.59485531, 0.67202572]),\n",
       " 'test_roc_auc': array([0.79513889, 0.56739487, 0.61171393, 0.81331983, 0.59999518,\n",
       "        0.58357446, 0.62637442, 0.71879823, 0.68648727, 0.48519483,\n",
       "        0.64076968, 0.53845968, 0.59030189, 0.65562307, 0.60772417,\n",
       "        0.63606238, 0.63935185, 0.68550195, 0.81515595, 0.67080897,\n",
       "        0.41993177, 0.24261696, 0.108577  , 0.33092105, 0.5808624 ,\n",
       "        0.43459302, 0.57698643, 0.69462209, 0.57221415, 0.67172965])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logregcv = LogisticRegression(penalty=\"elasticnet\", max_iter=5000, solver=\"saga\")\n",
    "logregcv = LogisticRegression(penalty=\"none\", max_iter=3000, solver=\"lbfgs\")\n",
    "\n",
    "prec = lambda x, y: precision_score(x, y, average=None)\n",
    "rec = lambda x, y: recall_score(x, y, average=None)\n",
    "\n",
    "scores = cross_validate(\n",
    "            logregcv,\n",
    "            X,\n",
    "            Y,\n",
    "            cv=30,\n",
    "            n_jobs=8,\n",
    "            scoring=['accuracy',\n",
    "                     'balanced_accuracy',\n",
    "                     'f1_weighted',\n",
    "                     'precision_weighted',3\n",
    "                     'recall_weighted', \"roc_auc\"],\n",
    "            groups=mldf.target,\n",
    "            verbose=1\n",
    "        )  # parallel processing\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot([0.71486957, 0.70836237, 0.70410114, 0.6949289 , 0.7141457 ])\n",
    "sns.distplot([0.76886035, 0.74371322, 0.7693954 , 0.78491172, 0.76445396])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0619874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced acc: (0.5652904709850635, 0.6419416111582794)\n",
      "accuracy    : (0.603115695930947, 0.6940707992459018)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "bal_acc = scores[\"test_balanced_accuracy\"]\n",
    "acc = scores[\"test_accuracy\"]\n",
    "\n",
    "\n",
    "print(\"balanced acc:\", st.t.interval(0.95, len(bal_acc)-1, loc=np.mean(bal_acc), scale=st.sem(bal_acc)))\n",
    "print(\"accuracy    :\", st.t.interval(0.95, len(acc)-1, loc=np.mean(acc), scale=st.sem(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "054e4a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time \t (141.536, 154.402)\n",
      "score_time \t (0.024, 0.035)\n",
      "test_accuracy \t (0.602, 0.693)\n",
      "test_balanced_accuracy \t (0.566, 0.642)\n",
      "test_f1_weighted \t (0.607, 0.693)\n",
      "test_precision_weighted \t (0.629, 0.703)\n",
      "test_recall_weighted \t (0.602, 0.693)\n",
      "test_roc_auc \t (0.529, 0.645)\n"
     ]
    }
   ],
   "source": [
    "for k, v in scores.items():\n",
    "    print(k, \"\\t\", tuple(map(lambda x: round(x, 3), st.t.interval(0.95, len(v)-1, loc=np.mean(v), scale=st.sem(v)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9265911f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_absolute_percentage_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'rand_score',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'top_k_accuracy',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f62e1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4432ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand-made confidence intervals\n",
    "\n",
    "bal_accs = []\n",
    "accs = []\n",
    "\n",
    "for _ in tqdm(range(20)):\n",
    "    words_shuffled = np.random.permutation(mldf.target.unique())\n",
    "\n",
    "    ratio = int(mldf.target.unique().shape[0]*0.8)\n",
    "    train_words = set(words_shuffled[:ratio])\n",
    "    msk = mldf.target.isin(train_words)\n",
    "\n",
    "    # msk = np.random.rand(len(mldf)) < 0.7\n",
    "\n",
    "    train_df = mldf[msk]\n",
    "    test_df = mldf[~msk]\n",
    "\n",
    "\n",
    "    target_train, anno_train, Y_train = (np.stack(train_df.target.apply(lambda w: scaled[mat_inds[w]])), \n",
    "                                         train_df.iloc[:, 5:].to_numpy(), \n",
    "                                         train_df.maj_vote.to_numpy())\n",
    "\n",
    "    target_test, anno_test, Y_test = (np.stack(test_df.target.apply(lambda w: scaled[mat_inds[w]])),\n",
    "                                        test_df.iloc[:, 5:].to_numpy(),\n",
    "                                        test_df.maj_vote.to_numpy())\n",
    "\n",
    "    X_train, X_test = np.hstack([target_train, anno_train]), np.hstack([target_test, anno_test])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    logreg = LogisticRegression(penalty=\"none\", max_iter=5000, solver=\"lbfgs\")\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Logistic Regression\n",
    "\n",
    "\n",
    "    preds = logreg.predict(X_test)\n",
    "\n",
    "    print(classification_report(Y_test, preds))\n",
    "    b = balanced_accuracy_score(Y_test, preds)\n",
    "    a = accuracy_score(Y_test,\n",
    "                       \n",
    "                       preds)\n",
    "    bal_accs.append(b)\n",
    "    accs.append(a)\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(Y_test, preds)\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plot_roc_curve(logreg, X_test, Y_test)\n",
    "    _=plt.plot([0,1], [0,1], \"--\", c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1b269",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b352a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, accuracy_score,\\\n",
    "                                confusion_matrix, plot_roc_curve\n",
    "\n",
    "preds = logreg.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, preds))\n",
    "print(balanced_accuracy_score(Y_test, preds))\n",
    "print(accuracy_score(Y_test, preds))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(Y_test, preds)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "plot_roc_curve(logreg, X_test, Y_test)\n",
    "_=plt.plot([0,1], [0,1], \"--\", c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6cf9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(bal_accs)\n",
    "\n",
    "sns.distplot(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c948ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
